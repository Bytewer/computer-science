# 梯度下降法

---

## 简介

* 不是一个机器学习算法
* 是一种基于搜哦所的最优化方法
* 作用：最小化一个损失函数
* 梯度上升法：最大化一个效用函数

![](.\img\6-1.png)

* η 称为学习率（leaning rate）
* η 的取值影响获得最优解的速度
* η 取值不合适，甚至得不到最优解
* η 是梯度下降法的一个超参数

![](.\img\6-2.png)

![](.\img\6-3.png)

* 并不是所有的函数都有唯一的极值点
  * 解决方案：
    * 多次运行，随机化初始点
    * 梯度下降法的初始点也是一个超参数

## 线性回归中的梯度下降法

![](.\img\6-4.png)

![](.\img\6-5.png)

目标：使以下式子尽可能的小：
$$
\sum \limits_{i=1}^m (y^{(i)} - \hat y^{(i)})^2
$$

$$
\hat y^{(i)} = \theta_0 + \theta_1 X_1^{(i)} + \theta_2 X_2^{(i)} + ... + \theta_n X_n^{(i)}
$$

所以目标：使以下式子尽可能的小
$$
\sum \limits_{i=1}^m (y^{(i)} - \theta_0 - \theta_1 X_1^{(i)} - \theta_2 X_2^{(i)} - ... - \theta_n X_n^{(i)})^2
$$
通过向量的方法求式子：

* 现在是θ作为变量，X作为常数
* 对每一个θ求偏导得到J

![](.\img\6-6.png)

除以一个m作为目标函数：

* 有时可以除以一个2 作为目标函数

![](.\img\6-7.png)

## 使用向量的方法解决线性回归中的梯度下降法

对上述推到的式子再进行向量化可得到：

![](.\img\6-8.png)

至此，推导的**梯度下降法公式**如下：

![](.\img\6-9.png)

## 随机梯度下降法

### 批量梯度下降法  Batch Gradient Descent

对于批量梯度下降法来说，终止的条件有两个：

1. 循环次数达到的损失函数的最大次数（n_iters）
2. 损失函数减少的值已经不能再减少我们预设的精度这么多了

如果m的值很大，带来的问题就是计算很耗时

![](.\img\6-10.png)

通过搜索找到损失函数的最小值

![](.\img\6-11.png)

### 随机梯度下降法 Stochastic Gradient Descent

相较于一般的梯度下降法，一般的是由固定的一个值一直到损失函数的最小值，如下图所示：

![](.\img\6-14.png)

通过随机梯度下降也可以大致来到损失函数的最小值位置附近

![](.\img\6-12.png)

学习率的取值很重要 η（模拟退火思想）：

* 模拟退火：打造铁的温度是从高到低

![](.\img\6-13.png)

在随机梯度下降法中不需要这个条件的判断：

```python
if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):
    break
```

这个判断是两次搜索的差距特别小就退出循环，这个在随机梯度下降中并不能体现出来，跟梯度有关。

## 关于梯度的调试

要求一个点相应的梯度值，首先要求它的导数（即点相切的直线）：

![](.\img\6-15.png)

可以模拟这个切线，方法就是首先在这个点的右边取一个点，再再负方向取一个点：

![](.\img\6-16.png)

如果要求红色的点切线，完全可以使用蓝色连线求极限代替

![](.\img\6-17.png)

​	对于高维的场景，对于每一个θ，都有一个对应的梯度：

![](.\img\6-18.png)

如果要对θ0进行求导就可以：

![](.\img\6-19.png)

同理θ1

![](.\img\6-20.png)

### 代码方式实现这个调试工具类

```python
def dJ_debug(theta, X_b, y, epsilon=0.01):
    res = np.empty(len(theta))
    for i in range(len(theta)):
        theta_1 = theta.copy()
        theta_1[i] += epsilon
        theta_2 = theta.copy()
        theta_2[i] -= epsilon
        res[i] = (J(theta_1, X_b, y) - J(theta_2, X_b, y)) / (2 * epsilon)
    return res
```

