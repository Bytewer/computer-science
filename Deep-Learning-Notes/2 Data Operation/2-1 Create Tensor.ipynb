{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 数据操作\n",
    "---\n",
    "\"tensor\"这个单词一般可译作“张量”，张量可以看作是一个多维数组。标量可以看作是0维张量，向量可以看作1维张量，矩阵可以看作是二维张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 创建Tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "创建一个5x3的未初始化的Tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.2755e-39, 1.0837e-38, 8.4490e-39],\n",
      "        [1.1112e-38, 1.0194e-38, 9.0919e-39],\n",
      "        [8.7245e-39, 8.4489e-39, 9.6429e-39],\n",
      "        [8.4490e-39, 9.6429e-39, 9.2755e-39],\n",
      "        [1.0286e-38, 9.0919e-39, 8.9082e-39]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "创建一个5x3的随机初始化的Tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8427, 0.3978, 0.3781],\n",
      "        [0.9266, 0.9541, 0.3382],\n",
      "        [0.8265, 0.4127, 0.1807],\n",
      "        [0.3206, 0.6836, 0.5329],\n",
      "        [0.3361, 0.5693, 0.4444]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "创建一个5x3的long型全0的Tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "直接根据数据创建"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "通过现有的Tensor来创建，此方法会默认重用输入Tensor的一些属性，例如数据类型，除非自定义数据类型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], dtype=torch.float64)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回的tensor默认具有相同的torch.dtype和torch.device\n",
    "x = x.new_ones(5, 3, dtype=torch.float64)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.7552, 0.3201, 0.7086],\n        [0.5939, 0.9713, 0.6202],\n        [0.8617, 0.7830, 0.6377],\n        [0.5623, 0.3388, 0.8145],\n        [0.5055, 0.0037, 0.3862]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rand_like:返回一个与输入相同大小的张量，该张量由均值为0、方差为1的正态分布的随机数填充\n",
    "x = torch.rand_like(x, dtype=torch.float)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "通过shape或者size()来获取Tensor的形状"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([5, 3])\n",
      "size:  torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"shape: \", x.shape)\n",
    "print(\"size: \", x.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "对角线为1，其他为0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.eye(3)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "从s到e，步长为step"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(1, 10, 1)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "从s到e，均匀切分成steps份"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-10.,  -5.,   0.,   5.,  10.])\n",
      "tensor([-10.])\n",
      "tensor([-10.,  10.])\n",
      "tensor([-10.0000,  -3.3333,   3.3333,  10.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.linspace(-10, 10, 5)\n",
    "print(x)\n",
    "y = torch.linspace(-10, 10, 1)\n",
    "print(y)\n",
    "z = torch.linspace(-10, 10, 2)\n",
    "print(z)\n",
    "v = torch.linspace(-10, 10, 4)\n",
    "print(v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "均匀/标准分布\n",
    "Returns a tensor filled with random numbers from a uniform distribution on the interval [0, 1)[0,1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6365, 0.4761, 0.0628, 0.9736])\n",
      "tensor([[0.7196, 0.5166, 0.9913],\n",
      "        [0.7014, 0.7980, 0.4820]])\n"
     ]
    }
   ],
   "source": [
    "# 返回一个由区间[0,1)[0,1]上的均匀分布的随机数组成的张量\n",
    "x = torch.rand(4)\n",
    "print(x)\n",
    "y = torch.rand(2, 3)\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "正态分布\n",
    "normal:返回一个从独立的正态分布中抽取的随机数的张量，其平均值和标准差已给定\n",
    "* mean:是一个张量，包含每个输出元素的正态分布的平均值\n",
    "* std:是一个张量，包含每个输出元素的正态分布的标准差"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1.0000, 0.9000, 0.8000, 0.7000, 0.6000, 0.5000, 0.4000, 0.3000, 0.2000,\n        0.1000])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1, 0, -0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1., 11.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.9345, 2.8076, 4.0208, 3.9346, 4.1944, 6.1756, 7.7775, 8.3139, 9.1035,\n",
      "        9.9307])\n",
      "tensor([ 0.4745,  0.7276, -2.7955, -3.4684, -0.6250])\n",
      "tensor([-0.2344,  2.5589,  4.7012,  3.8044,  6.9503])\n",
      "tensor([[2.3079, 3.5329, 2.2496, 4.3742]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))\n",
    "#第一个数字是从均值为1，标准差为1的正态分布中随机生成的\n",
    "#第二个是从均值为2，标准差为0.9的正态分布中随机生成\n",
    "print(x)\n",
    "y = torch.normal(mean=0.5, std=torch.arange(1., 6.))\n",
    "print(y)\n",
    "z = torch.normal(mean=torch.arange(1., 6.))\n",
    "print(z)\n",
    "v = torch.normal(2, 3, size=(1, 4))\n",
    "print(v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "randperm 随机置换\n",
    "返回一个从0到n-1的整数的随机置换"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([4, 8, 2, 9, 1, 3, 5, 6, 7, 0])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randperm(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 操作"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 算数操作"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.5615, 0.6131, 0.5585],\n        [0.5437, 0.9413, 0.2551],\n        [0.6378, 0.6153, 0.6793],\n        [0.4757, 0.7385, 0.8357],\n        [0.2890, 0.3955, 0.4032]])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.new_ones(5, 3, dtype=torch.float64)\n",
    "x = torch.randn_like(x, dtype=torch.float)\n",
    "y = torch.rand(5, 3)\n",
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9888, -0.2924, -0.4634],\n",
      "        [ 2.6660,  1.4676,  0.5621],\n",
      "        [-0.3135,  0.4980, -0.5778],\n",
      "        [ 1.4999,  2.1278,  0.6126],\n",
      "        [ 1.1733, -0.0397, -0.8265]])\n"
     ]
    }
   ],
   "source": [
    "print(x + y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.9888, -0.2924, -0.4634],\n        [ 2.6660,  1.4676,  0.5621],\n        [-0.3135,  0.4980, -0.5778],\n        [ 1.4999,  2.1278,  0.6126],\n        [ 1.1733, -0.0397, -0.8265]])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.9888, -0.2924, -0.4634],\n        [ 2.6660,  1.4676,  0.5621],\n        [-0.3135,  0.4980, -0.5778],\n        [ 1.4999,  2.1278,  0.6126],\n        [ 1.1733, -0.0397, -0.8265]])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(x)\n",
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 索引\n",
    "可以使用类似NumPy的索引操作来访问Tensor的一部分，需要注意的是：索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5502, -0.9055, -1.0219],\n",
      "        [ 2.1223,  0.5264,  0.3070],\n",
      "        [-0.9514, -0.1173, -1.2571],\n",
      "        [ 1.0242,  1.3893, -0.2232],\n",
      "        [ 0.8842, -0.4352, -1.2297]])\n",
      "tensor([-1.5502, -0.9055, -1.0219])\n",
      "tensor([-0.5502,  0.0945, -0.0219])\n",
      "tensor([-0.5502,  0.0945, -0.0219])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "y = x[0, :]\n",
    "print(y)\n",
    "y += 1\n",
    "print(y)\n",
    "print(x[0, :])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "index_select(input, dim, index)在指定维度dim上选取，比如选取某些行、某些列\n",
    "* input (Tensor) – the input tensor.\n",
    "* dim (int) – the dimension in which we index\n",
    "* index (IntTensor or LongTensor) – the 1-D tensor containing the indices to index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 2])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.tensor([0, 2])\n",
    "indices"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-0.5502, -0.0219])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.index_select(y, 0, indices)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "masked_select(input, mask) 返回一个新的一维张量，该张量根据布尔掩码Mask对输入张量进行索引，而Mask是一个BoolTensor\n",
    "* input (Tensor) – the input tensor.\n",
    "* mask (BoolTensor) – the tensor containing the binary mask to index with"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.0737, -0.2925, -0.2399,  1.0978],\n        [-0.3030,  0.3558, -0.8019,  2.2121],\n        [-0.9186, -0.2112,  1.3251, -0.5383]])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.randn(3, 4)\n",
    "v"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ True, False, False,  True],\n        [False, False, False,  True],\n        [False, False,  True, False]])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = v.ge(0.5)\n",
    "mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1.0737, 1.0978, 2.2121, 1.3251])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.masked_select(v, mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "nonzero(input)\t非0元素的下标"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.]])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.eye(3, 4)\n",
    "w"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 0],\n        [1, 1],\n        [2, 2]])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero(w)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "gather(input, dim, index)\t 沿给定轴dim，将输入索引张量index指定位置的值进行聚合\n",
    "理解：https://www.cnblogs.com/HongjianChen/p/9451526.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 2],\n        [3, 4]])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果dim=0，那么它表示的就是你接下来的操作是对于第一维度进行的，也就是行；如果dim=1,那么它表示的就是你接下来的操作是对于第二维度进行的，也就是列\n",
    "t = torch.tensor([[1,2], [3,4]])\n",
    "t"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2, 1],\n        [4, 3]])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.gather(t,1,torch.LongTensor([[1,0],[1,0]]))\n",
    "b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2, 2],\n        [4, 3]])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.gather(t,1,torch.LongTensor([[1,1],[1,0]]))\n",
    "b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 2],\n        [1, 4]])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.gather(t, 0, torch.tensor([[0,0], [0,1]]))\n",
    "b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15]])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(0, 16).view(4, 4)\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0,  5, 10, 15]])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = torch.tensor([[0,1,2,3]])\n",
    "a.gather(0, index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 1, 2, 3]])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.gather(1, index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 改变形状"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5502,  0.0945, -0.0219],\n",
      "        [ 2.1223,  0.5264,  0.3070],\n",
      "        [-0.9514, -0.1173, -1.2571],\n",
      "        [ 1.0242,  1.3893, -0.2232],\n",
      "        [ 0.8842, -0.4352, -1.2297]])\n",
      "tensor([-0.5502,  0.0945, -0.0219,  2.1223,  0.5264,  0.3070, -0.9514, -0.1173,\n",
      "        -1.2571,  1.0242,  1.3893, -0.2232,  0.8842, -0.4352, -1.2297])\n",
      "tensor([[-0.5502,  0.0945, -0.0219,  2.1223,  0.5264],\n",
      "        [ 0.3070, -0.9514, -0.1173, -1.2571,  1.0242],\n",
      "        [ 1.3893, -0.2232,  0.8842, -0.4352, -1.2297]])\n",
      "torch.Size([5, 3]) torch.Size([15]) torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "y = x.view(15)\n",
    "z = x.view(-1, 5)  # -1所指的维度可以根据其他维度的值推出来\n",
    "print(y)\n",
    "print(z)\n",
    "print(x.size(), y.size(), z.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4498,  1.0945,  0.9781],\n",
      "        [ 3.1223,  1.5264,  1.3070],\n",
      "        [ 0.0486,  0.8827, -0.2571],\n",
      "        [ 2.0242,  2.3893,  0.7768],\n",
      "        [ 1.8842,  0.5648, -0.2297]])\n",
      "tensor([ 0.4498,  1.0945,  0.9781,  3.1223,  1.5264,  1.3070,  0.0486,  0.8827,\n",
      "        -0.2571,  2.0242,  2.3893,  0.7768,  1.8842,  0.5648, -0.2297])\n"
     ]
    }
   ],
   "source": [
    "x += 1\n",
    "print(x)\n",
    "print(y) # 也加了1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pytorch还提供了一个reshape()可以改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用。推荐先用clone创造一个副本然后再使用view"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5502,  0.0945, -0.0219],\n",
      "        [ 2.1223,  0.5264,  0.3070],\n",
      "        [-0.9514, -0.1173, -1.2571],\n",
      "        [ 1.0242,  1.3893, -0.2232],\n",
      "        [ 0.8842, -0.4352, -1.2297]])\n",
      "tensor([ 0.4498,  1.0945,  0.9781,  3.1223,  1.5264,  1.3070,  0.0486,  0.8827,\n",
      "        -0.2571,  2.0242,  2.3893,  0.7768,  1.8842,  0.5648, -0.2297])\n"
     ]
    }
   ],
   "source": [
    "# 使用clone还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源Tensor。\n",
    "x_cp = x.clone().view(15)\n",
    "x -= 1\n",
    "print(x)\n",
    "print(x_cp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "item(), 它可以将一个标量Tensor转换成一个Python number"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0153])\n",
      "-0.015278167091310024\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 线性代数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2., 1., 1.],\n        [1., 2., 1.],\n        [1., 1., 2.]])"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.eye(3) + 1\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(6.)"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对角线元素之和(矩阵的迹)\n",
    "torch.trace(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([3., 3., 3.])"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对角线元素\n",
    "x = x.clone() + 1\n",
    "torch.diag(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.7511, -0.2346,  1.0392],\n        [ 0.7944,  0.5402, -1.3615],\n        [-2.1461, -0.4689,  0.1641]])"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵的上三角/下三角，可指定偏移量\n",
    "# triu: 返回矩阵（二维张量）的上三角部分或输入的一批矩阵，结果张量的其他元素被设置为0\n",
    "# tril\n",
    "x = torch.randn(3,3)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.7511, -0.2346,  1.0392],\n        [ 0.0000,  0.5402, -1.3615],\n        [ 0.0000,  0.0000,  0.1641]])"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.0000, -0.2346,  1.0392],\n        [ 0.0000,  0.0000, -1.3615],\n        [ 0.0000,  0.0000,  0.0000]])"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# diagonal参数 (int, optional) – “1“对角线向右上一次，\"-1\"对角线向左下一次\n",
    "torch.triu(x, diagonal=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.8090, -1.1776,  0.4133,  0.1061, -1.2073],\n        [-2.1458, -0.4703, -0.2549, -0.2104,  0.3840],\n        [ 0.0000, -0.7530, -0.0139,  0.5775,  0.9320],\n        [ 0.0000,  0.0000, -2.2893, -1.9402,  1.1488],\n        [ 0.0000,  0.0000,  0.0000, -0.3985, -0.1751]])"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5, 5)\n",
    "torch.triu(x, diagonal=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 2],\n        [3, 4]])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mm/bmm 矩阵乘法，batch的矩阵乘法\n",
    "x = torch.tensor([[1,2],[3,4]])\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[5, 6],\n        [7, 8]])"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([[5,6],[7,8]])\n",
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[19, 22],\n        [43, 50]])"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(x, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0.9632, 0.0296, 0.9487, 0.0221],\n         [0.1470, 0.0163, 0.1988, 0.6000],\n         [0.4407, 0.8579, 0.1926, 0.3959]],\n\n        [[0.9962, 0.1716, 0.0696, 0.1922],\n         [0.1825, 0.9752, 0.7741, 0.9751],\n         [0.6935, 0.6885, 0.1900, 0.6347]],\n\n        [[0.7103, 0.9228, 0.1450, 0.3555],\n         [0.8738, 0.6252, 0.3647, 0.3361],\n         [0.8122, 0.4566, 0.2099, 0.0398]],\n\n        [[0.2573, 0.3206, 0.0261, 0.3618],\n         [0.6330, 0.1651, 0.3888, 0.3686],\n         [0.7123, 0.1138, 0.5415, 0.5213]],\n\n        [[0.7184, 0.6829, 0.9144, 0.4189],\n         [0.3127, 0.1605, 0.2438, 0.7043],\n         [0.8717, 0.7632, 0.0868, 0.5055]],\n\n        [[0.2046, 0.1821, 0.2941, 0.0775],\n         [0.3385, 0.4854, 0.3297, 0.1493],\n         [0.8363, 0.8132, 0.3609, 0.6692]],\n\n        [[0.6849, 0.8005, 0.6520, 0.1599],\n         [0.9816, 0.2925, 0.4066, 0.2128],\n         [0.2195, 0.3144, 0.2010, 0.4741]],\n\n        [[0.1778, 0.3487, 0.5145, 0.6161],\n         [0.3364, 0.5266, 0.1605, 0.7420],\n         [0.0600, 0.9505, 0.7978, 0.0515]],\n\n        [[0.8230, 0.5463, 0.5969, 0.0219],\n         [0.3799, 0.0840, 0.6672, 0.1043],\n         [0.2094, 0.6135, 0.8842, 0.0132]],\n\n        [[0.9090, 0.6483, 0.1510, 0.5332],\n         [0.3046, 0.9037, 0.0389, 0.0675],\n         [0.1773, 0.8856, 0.0358, 0.6375]]])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.rand(10, 3, 4)\n",
    "input"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 3, 5])"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = torch.randn(10, 4, 5)\n",
    "res = torch.bmm(input, mat)\n",
    "res.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 2],\n        [3, 4]])"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t\t转置\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 3],\n        [2, 4]])"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.t(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(7)"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 与NumPy的dot不同，torch.dot故意只支持计算两个元素数量相同的一维张量的点乘\n",
    "# dot/cross\t内积/外积\n",
    "torch.dot(torch.tensor([2,3]), torch.tensor([2,1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1, 2, 3])"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross注意事项：\n",
    "# input和other必须有相同的尺寸，并且其dim维度的尺寸应该是3。\n",
    "# 如果没有给出dim，则默认为找到的第一个尺寸为3的尺寸。\n",
    "x = torch.tensor([1,2,3])\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([4, 5, 6])"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([4,5,6])\n",
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-3,  6, -3])"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#？？？？？？？？？？？？？？？？？？？？？？？？？\n",
    "torch.cross(x, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(32)"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(x, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "# svd\t奇异值分解\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 广播机制\n",
    "当对两个形状不同的Tensor按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个Tensor形状相同后再按元素运算"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# 由于x和y分别是1行2列和3行1列的矩阵，如果要计算x + y，那么x中第一行的2个元素被广播（复制）到了第二行和第三行，而y中第一列的3个元素被广播（复制）到了第二列。如此，就可以对2个3行2列的矩阵按元素相加\n",
    "x = torch.arange(1,3).view(1,2)\n",
    "print(x)\n",
    "y = torch.arange(1,4).view(3,1)\n",
    "print(y)\n",
    "print(x + y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 运算的内存开销\n",
    "前面说了，索引操作是不会开辟新内存的，而像y = x + y这样的运算是会新开内存的，然后将y指向新内存。为了演示这一点，我们可以使用Python自带的id函数：如果两个实例的ID一致，那么它们所对应的内存地址相同；反之则不同"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1,2])\n",
    "y = torch.tensor([3,4])\n",
    "before_id = id(y)\n",
    "y = x + y\n",
    "print(id(y) == before_id)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# 把x + y的结果通过[:]写进y对应的内存中\n",
    "x = torch.tensor([1,2])\n",
    "y = torch.tensor([3,4])\n",
    "before_id = id(y)\n",
    "y[:] = x + y\n",
    "print(id(y) == before_id)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# 可以使用运算符全名函数中的out参数或者自加运算符+=(也即add_())达到上述效果，例如torch.add(x, y, out=y)和y += x(y.add_(x))\n",
    "x = torch.tensor([1,2])\n",
    "y = torch.tensor([3,4])\n",
    "before_id = id(y)\n",
    "torch.add(x, y, out=y) # y += x, y.add_(x)\n",
    "print(id(y) == before_id)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensor和Numpy相互转换"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tensor转NumPy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1., 1., 1., 1., 1.])"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1., 1., 1., 1., 1.], dtype=float32)"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2., 2., 2., 2., 2.])"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a += 1\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "array([3., 3., 3., 3., 3.], dtype=float32)"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b += 1\n",
    "b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NumPy数组转Tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1, 1, 1, 1, 1], dtype=torch.int32)"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5, dtype='int')\n",
    "b = torch.from_numpy(a)\n",
    "b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "array([2, 2, 2, 2, 2])"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a += 1\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([3, 3, 3, 3, 3], dtype=torch.int32)"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b += 1\n",
    "b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([3, 3, 3, 3, 3], dtype=torch.int32)"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接通过构造函数创建\n",
    "c = torch.tensor(a)\n",
    "c"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 自动求梯度\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![1](./img/Snipaste_2021-09-20_17-51-54.png)\n",
    "* grad: 该Tensor的梯度值，每次在计算backward时需要将前一时刻的梯度归零，否则梯度会一直累加\n",
    "* grad_fn: 叶子节点通常为None，只有结果节点的grad_fn才有效，用于指示梯度函数时哪种类型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "print(x.grad_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x000001AF8F3B7760>\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# x是直接创建的，所以它没有grad_fn; y是通过一个加法操作创建的，所以它有一个为<AddBackward>的grad_fn\n",
    "# 像x这种直接创建的称为叶子节点，叶子节点对应的grad_fn是None\n",
    "\n",
    "# 只有叶子张量有梯度\n",
    "print(x.is_leaf)\n",
    "print(y.is_leaf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "<built-in method mean of Tensor object at 0x000001AF8F3B47C0>\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean\n",
    "print(z)\n",
    "print(out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x000001AF8F3B7790>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 梯度"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
    "y = 2 * x\n",
    "z = y.view(2, 2)\n",
    "\n",
    "print(z)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "backward():\n",
    "* tensor: 用于计算梯度的tensor\n",
    "* grad_tensor: 在计算矩阵的梯度时会用到。他其实也是一个tensor,shape一般需要和前面的tensor保持一致\n",
    "* retain_graph: 通常调用一次backward后，pytorch会自动把计算图销毁，所以要想对某个变量重复调用backward，则需要将该参数设置为True\n",
    "* create_graph: 如果为True，那么就创建一个专门的graph of the derivative, 这可以方便计算高阶微分\n",
    "\n",
    "grad():计算和返回output关于inputs的梯度和\n",
    "* outputs: 函数的因变量，即需要求导的那个函数\n",
    "* input: 函数的自变量，可以定义多个tensor\n",
    "* grad_outputs: 同backward\n",
    "* only_inputs: 只计算input的梯度\n",
    "* allow_unused(bool 可选)：如果为false，当计算输出出错时（因此他们的梯度永远为0）指明不适应inputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0000, 0.2000, 0.0200, 0.0020])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([[1.0, 0.1], [0.01, 0.001]], dtype=torch.float)\n",
    "z.backward(v)\n",
    "print(x.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(1., grad_fn=<PowBackward0>) True\n",
      "tensor(1.) False\n",
      "tensor(2., grad_fn=<AddBackward0>) True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y1 = x ** 2\n",
    "with torch.no_grad():\n",
    "    y2 = x ** 3\n",
    "y3 = y1 + y2\n",
    "\n",
    "print(x.requires_grad)\n",
    "print(y1, y1.requires_grad)\n",
    "print(y2, y2.requires_grad)\n",
    "print(y3, y3.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "y3.backward()\n",
    "print(x.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "# y2.backward() # 报错"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "False\n",
      "tensor([100.], requires_grad=True)\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "# 如果我们想要修改tensor的数值，但是又不希望被autograd记录（即不会影响反向传播），那么我么可以对tensor.data进行操作。\n",
    "x = torch.ones(1, requires_grad=True)\n",
    "print(x.data)\n",
    "print(x.data.requires_grad)\n",
    "\n",
    "y = x * 2\n",
    "x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播\n",
    "\n",
    "y.backward()\n",
    "print(x)\n",
    "print(x.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "torch.autograd.Function\n",
    "每一个原始的自动求导运算实际上是两个在Tensor上运行的函数\n",
    "* forward函数计算从输入Tensor获得的输出Tensors\n",
    "* backward函数接收了输出Tensor对于某个标量值的梯度，并且计算输入Tensors相对于该相同标量值的梯度\n",
    "* 最后，利用apply方法执行相应的运算"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14., grad_fn=<SumBackward0>)\n",
      "tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1.0,2.0,3.0], requires_grad=True)\n",
    "y = x ** 2\n",
    "print(y.sum())\n",
    "# y.backward() ×\n",
    "y.sum().backward()\n",
    "print(x.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [4, 5, 6],\n",
      "        [5, 6, 7]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1,4).view(1,3)\n",
    "print(x)\n",
    "y = torch.arange(1,5).view(4,1)\n",
    "print(y)\n",
    "print(x + y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}